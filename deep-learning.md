#Deep Learning
* [Reducing the Dimensionality of Data with Neural Networks](http://www.cs.toronto.edu/~hinton/science.pdf) - Deep Learning の火付け役となった論文
* [ajtulloch/dnngraph](https://github.com/ajtulloch/dnngraph)
* [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)
* [Deep Learningと音声認識](https://research.preferred.jp/2015/07/deep-learning-speech-recognition/)
* [自然言語処理のためのDeep Learning](http://www.slideshare.net/yutakikuchi927/deep-learning-26647407)
* [Deep learning via Hessian-free optimization](http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_Martens10.pdf)
* [Deep learning](http://www.slideshare.net/kazoo04/deep-learning-15097274)
* [一般向けのDeep Learning](http://www.slideshare.net/pfi/deep-learning-22350063)
* [andrewt3000/DL4NLP](https://github.com/andrewt3000/DL4NLP)
* [New ‘deep learning’ technique enables robot mastery of skills via trial and error](http://newscenter.berkeley.edu/2015/05/21/deep-learning-robot-masters-skills-via-trial-and-error/)
* [ニコニコ動画の公開コメントデータをDeep Learningで解析する](http://qiita.com/ixixi/items/a3d56b2db6e09249a519)
* [深層学習ライブラリのプログラミングモデル](http://www.slideshare.net/yutakashino/ss-56291783)
* [DeepLearningを使った実装を纏めてみた](http://nonbiri-tereka.hatenablog.com/entry/2015/12/17/004410)
* [計算グラフの微積分：バックプロパゲーションを理解する](http://postd.cc/2015-08-backprop/)
* [誤差逆伝播法のノート](http://qiita.com/Ugo-Nama/items/04814a13c9ea84978a4c)
* [【エヴァンゲリオン】アスカっぽいセリフをDeepLearningで自動生成してみる](http://qiita.com/S346/items/24e875e3c5ac58f55810)
* [深層学習でツイートの感情分析](http://qiita.com/hogefugabar/items/93fcb2bc27d7b268cbe6)
* [深層学習でニュース記事を分類する](http://qiita.com/hogefugabar/items/c27ed578717c5e7288c0)
* [DQNの生い立ち＋Deep Q-NetworkをChainerで書いた](http://qiita.com/Ugo-Nama/items/08c6a5f6a571335972d5)
* [DeepLearningの基礎と応用事例](http://www.slideshare.net/Takayosi/deep-learning-seminar)
* [ニューラルネットワークと深層学習](http://nnadl-ja.github.io/nnadl_site_ja/index.html)
* [DeepLearning〜使いこなすために知っておきたいこと〜](http://www.slideshare.net/Takayosi/miru2014-tutorial-deeplearning-37219713)
* [ディープラーニングによる画像認識と応用事例](http://www.slideshare.net/Takayosi/nvidia-51814334)
* [Deep Learningと画像認識～歴史・理論・実践～](http://www.slideshare.net/nlab_utokyo/deep-learning-40959442)
* [Deep Learningによる画像認識革命　ー歴史・最新理論から実践応用までー](http://www.slideshare.net/nlab_utokyo/deep-learning-49182466)
* [ディープラーニングの基礎技術と今後の課題・展望〜画像認識分野を中心に〜](http://www.slideshare.net/nlab_utokyo/20150930-53741757)
* [DeepLearningの過去と未来〜黒魔術からの脱却へ向けて〜](http://www.slideshare.net/nlab_utokyo/20150414seminar)
* [画像認識分野でのディープラーニングの研究動向](http://ibisml.org/archive/ibis2013/pdfs/ibis2013-okatani.pdf)
* [一般向けのDeep Learning](http://www.slideshare.net/pfi/deep-learning-22350063)
* [Deep Learningの基礎と応用](http://www.slideshare.net/beam2d/deep-learning-52872945)
* [Deep learning実装の基礎と実践](http://www.slideshare.net/beam2d/deep-learningimplementation)
* [日本神経回路学会セミナー「DeepLearningを使ってみよう！」資料](http://www.slideshare.net/KentaOono/deeplearning-52072261)
* [ディープラーニング チュートリアル（もしくは研究動向報告）](http://www.vision.is.tohoku.ac.jp/files/9313/6601/7876/CVIM_tutorial_deep_learning.pdf)
* [画像認識のための深層学習](http://www.slideshare.net/yomoyamareiji/ss-36982686)
* [なぜGPUはディープラーニングに向いているか](http://www.slideshare.net/NVIDIAJapan/gpu-51812528)
* [-SSIIの技術マップ- 過去•現在, そして未来 ［領域］認識](http://www.slideshare.net/hironobufujiyoshi/ssii-35734246)
* [論文輪読資料「A review of unsupervised feature learning and deep learning for time-series modeling」](http://www.slideshare.net/kaorunasuno/20150416-nasuno-ul)
* [Deep learning勉強会20121214ochi](http://www.slideshare.net/alembert2000/deep-learning20121214ochi)
* [Deep Learning 勉強会 (Chapter 7-12)](http://www.slideshare.net/alembert2000/deep-learning-chapter-712)
* [Learning Deep Architectures for AI （第 3 回 Deep Learning 勉強会資料; 松尾）](http://www.slideshare.net/alembert2000/learning-deep-architectures-for-ai-3-deep-learning)
* [Fast dropout training](http://jmlr.csail.mit.edu/proceedings/papers/v28/wang13a.pdf)
* [Dropoutの実装と重みの正則化](http://olanleed.hatenablog.com/entry/2013/12/03/010945)
* [Multimodal Deep Learning](http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Ngiam_399.pdf)
* [BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1](http://arxiv.org/abs/1602.02830)
* [BinaryNetとBinarized Deep Neural Network](http://tkng.org/b/2016/02/12/binarynet-and-binarized-neural-network/)
* [DeepLearningを使った実装を纏めてみた](http://nonbiri-tereka.hatenablog.com/entry/2015/12/17/004410)
* [3D Visualization of a Convolutional Neural Network](http://scs.ryerson.ca/~aharley/vis/conv/)

##Tools & Tutorials
* [Theano](http://deeplearning.net/software/theano/)
  * [Tutorial](http://deeplearning.net/software/theano/tutorial/)
  * [Theanoによる自己符号化器の実装](http://aidiary.hatenablog.com/entry/20151203/1449146680)
  * [DeepLearning Documentation](http://deeplearning.net/tutorial/contents.html)
  * [Theano の 基本メモ](http://qiita.com/mokemokechicken/items/3fbf6af714c1f66f99e9)
  * [Theano 解説](http://d.hatena.ne.jp/saket/20121207/1354867911)

###Chainer
> 制御構造はすべて Python のものがそのままつかえます。Chainer は、実際に Python のコードを用いて入力配列に何の処理が適用されたかだけを記憶しておき、それを誤差逆伝播の実行に使います。

[Deep Learning のフレームワーク Chainer を公開しました](https://research.preferred.jp/2015/06/deep-learning-chainer/)

* [Chainer](http://chainer.org/) - 公式ドキュメント
* [Chainer – A flexible framework of neural networks](http://docs.chainer.org/en/stable/)
* [Chainerの使い方と自然言語処理への応用](http://www.slideshare.net/unnonouno/chainer-55494686)
* [Chainerを使う場合の全体の流れ](http://www.slideshare.net/ryokuta/chainer-v15-view2015/29)
* [Chainerで顔イラストの自動生成](http://qiita.com/mattya/items/e5bfe5e04b9d2f0bbd47)
* [Chainerで学習した対話用のボットをSlackで使用+Twitterから学習データを取得してファインチューニング](http://qiita.com/GushiSnow/items/79ca7deeb976f50126d7)
* [Chainerでファインチューニングするときの個人的ベストプラクティス](http://qiita.com/tabe2314/items/6c0c1b769e12ab1e2614)
* [【機械学習】ディープラーニング フレームワークChainerを試しながら解説してみる。](http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412)
* [【ディープラーニング】ChainerでAutoencoderを試して結果を可視化してみる。](http://qiita.com/kenmatsu4/items/99d4a54d5a57405ecaf8)
* [Chainerを使ってコンピュータにイラストを描かせる](http://qiita.com/rezoolab/items/5cc96b6d31153e0c86bc)
* [Chainer入門と最近の機能](http://www.slideshare.net/unnonouno/chainer-56292907)
* [ChainerとRNNと機械翻訳](http://qiita.com/odashi_t/items/a1be7c4964fbea6a116e)
* [mattya/chainer-DCGAN](https://github.com/mattya/chainer-DCGAN)
* [Chainerを用いたマルウェア検出への取り組みについて](http://olanleed.hatenablog.com/entry/2015/12/13/215246)
* [ニューラルネットワークを用いたランク学習(ChainerによるRankNetの実装)](http://qiita.com/sz_dr/items/0e50120318527a928407)
* [Chainerを使った多層パーセプトロン：関数フィッティング](http://qiita.com/nga_m0m0/items/4191c0f847d1c749fd84)

###Tensor Flow
* [TensorFlow](http://tensorflow.org/)
* [Neural Networkをちょっとかじった人のための、はじめてのTensorFlow](http://qiita.com/sergeant-wizard/items/55256ac6d5d8d7c53a5a)
* [TensorFlow 畳み込みニューラルネットワークで手書き認識率99.2%の分類器を構築](http://qiita.com/haminiku/items/36982ae65a770565458d)
* [TensorFlow ってなんだろ](http://qiita.com/shuhei_f/items/5ba61fff4e47e073c24f)
* [TensorFlowとTensorBoardでニューラルネットワークを可視化](http://qiita.com/sergeant-wizard/items/fdf4d64a0d221a81da34)
* [TensorFlowのコード分割の考え方](http://qiita.com/sergeant-wizard/items/98ce0993a195475fd7a9)
* [TensorFlow紹介文の適当和訳 ななめ読み用](http://qiita.com/tomo_makes/items/af23c1ac0d94b764da55)
* [TesnsorFlowで深層学習像が変わる？少し触って思った事](http://qiita.com/n_kats_/items/ba7f19701e39d8ff1f6c)
* [TensorFlowを社内向けにざっくりLTして回帰した(＋資料とか)](http://yamitzky.hatenablog.com/entry/2015/11/11/204416)
* [nivwusquorum/tensorflow-deepq](https://github.com/nivwusquorum/tensorflow-deepq)
* [TensorFlowチュートリアル - TensorFlowメカニクス101（翻訳）](http://qiita.com/KojiOhki/items/0640d01029371d6ae092)
* [TensorFlowチュートリアル - 画像認識（翻訳）](http://qiita.com/KojiOhki/items/dab6922b6cd7b990c002)
* [TensorFlowをscikit-learnライクに使えるskflow](http://qiita.com/icoxfog417/items/9d052b556bd8a4074e4a)
* [TensorFlowでのMNIST学習結果を、実際に手書きして試す](http://d.hatena.ne.jp/sugyan/20151124/1448292129)
* [ディープラーニングでおそ松さんの六つ子は見分けられるのか 〜実施編〜](http://bohemia.hatenablog.com/entry/2015/11/22/174603)
* [TensorFlowによるディープラーニングで、アイドルの顔を識別する](http://d.hatena.ne.jp/sugyan/20160112/1452558576)
* [Googleから出た機械学習ライブラリTensorFlowのインストール](http://qiita.com/bohemian916/items/62ef3fe4d90745cc92f6)
* [Distributed TensorFlowの話](http://qiita.com/kazunori279/items/981a8a2a44f5d1172856)

##NN
* [jbarrow/LambdaNet](https://github.com/jbarrow/LambdaNet)
* [alpmestan/hnn](https://github.com/alpmestan/hnn)
* [Neural networks in Haskell (Lynn)](https://twitter.com/mcarberg/status/664750004742000640)
* [Haskellでニューラルネットワーク](http://imokuri123.com/blog/2015/07/neural-network-in-haskell.html)
* [数式で書き下す Maxout Networks](http://blog.yusugomori.com/post/133257383300/%E6%95%B0%E5%BC%8F%E3%81%A7%E6%9B%B8%E3%81%8D%E4%B8%8B%E3%81%99-maxout-networks)
* [ニューラルネットワーク、多様体、トポロジー](http://qiita.com/KojiOhki/items/af2241027b00f892d2bd)
* [ConvnetJS demo: toy 2d classification with 2-layer neural network](http://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html)
* [ロジスティック回帰 (勾配降下法 / 確率的勾配降下法) を可視化する](http://sinhrks.hatenablog.com/entry/2014/11/24/205305)

##CNN
* [畳み込みニューラルネット](http://www.slideshare.net/ssuser726f56/joi-51681753)
* [画風を変換するアルゴリズム](https://research.preferred.jp/2015/09/chainer-gogh/)
* [数式で書き下す Convolutional Neural Networks (CNN)](http://blog.yusugomori.com/post/129688163130/%E6%95%B0%E5%BC%8F%E3%81%A7%E6%9B%B8%E3%81%8D%E4%B8%8B%E3%81%99-convolutional-neural-networks-cnn)
* [深層畳み込みニューラルネットワークを用いた画像スケーリング](http://postd.cc/image-scaling-using-deep-convolutional-neural-networks-part1/)
* [ディープラーニングにおける様々な物体領域検出のアプローチ方法(R-CNN)](http://qiita.com/t-hiroyoshi/items/e9def50ba2c2249db04b)

##RNN
* [JPMoresmau/rnn](https://github.com/JPMoresmau/rnn)
* [リカレントニューラルネットなぜ強い？](http://d.hatena.ne.jp/mamoruk/20151209/p1)
* [Recurrent Neural Networks](http://www.slideshare.net/beam2d/pfi-seminar-20141030rnn)
* [Recurrent Neural Networks Tutorial, Part 1 – Introduction to RNNs](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)
* [Playing with Recurrent Neural Networks in Haskell](http://jpmoresmau.blogspot.jp/2015/08/playing-with-recurrent-neural-networks.html)
* [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
* [TensorFlowチュートリアル - リカレント・ニューラルネットワーク（翻訳）](http://qiita.com/KojiOhki/items/149f96bd98973bd219ac)
* [ChainerとRNNと機械翻訳](http://qiita.com/odashi_t/items/a1be7c4964fbea6a116e)
* [ニューラルネットワークで時系列データの予測を行う](http://qiita.com/icoxfog417/items/2791ee878deee0d0fd9c)
* [再帰型ニューラルネットワーク講義シリーズ・第1部: RNN入門](http://blog.moji.ai/2016/01/%E5%86%8D%E5%B8%B0%E5%9E%8B%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%AC%9B%E7%BE%A9%E3%82%B7%E3%83%AA%E3%83%BC%E3%82%BA%E3%83%BB%E7%AC%AC1/)
* [論文輪読資料「Gated Feedback Recurrent Neural Networks」](http://www.slideshare.net/kurotaki_weblab/gated-feedback-recurrent-neural-networks)
  * [最近のDeep Learning (NLP) 界隈におけるAttention事情](http://www.slideshare.net/yutakikuchi927/deep-learning-nlp-attention)

###LSTM
* [LSTMネットワークの概要](http://qiita.com/KojiOhki/items/89cd7b69a8a6239d67ca)
* [わかるLSTM ～ 最近の動向と共に](http://qiita.com/t_Signull/items/21b82be280b46f467d1b)
* [Chainerで学ぶLSTM](http://kivantium.hateblo.jp/entry/2016/01/31/222050)
* [RECURRENT NEURAL NETWORK REGULARIZATION](http://arxiv.org/pdf/1409.2329v4.pdf)
* [Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks](http://arxiv.org/abs/1503.00075)
* [Long Short-Term Memory in Recurrent Neural Networks](http://www.felixgers.de/papers/phd.pdf)

##Autoencoder
* [論文輪読資料「Why regularized Auto-Encoders learn Sparse Representation?」DL Hacks](http://www.slideshare.net/kurotaki_weblab/why-regularized-autoencoders-learn-sparse-representationdl-hacks)
* [ディープラーニング勉強会 AutoEncoder](http://qiita.com/piyo7/items/60576759430910ffe5be)
* [word2vec](https://code.google.com/p/word2vec/)
* [【ディープラーニング】ChainerでAutoencoderを試して結果を可視化してみる。](http://qiita.com/kenmatsu4/items/99d4a54d5a57405ecaf8)
* [Doc2Vecとk-meansで教師なしテキスト分類](http://qiita.com/shima_x/items/196e8d823412e45680e9)
* [Semi-Supervised Autoencoders for Predicting Sentiment Distributions（第 5 回 Deep Learning 勉強会資料; ダヌシカ）](http://www.slideshare.net/alembert2000/ssrae-sa)

##RBM
* [aeyakovenko/rbm](https://github.com/aeyakovenko/rbm)
* [RBMから考えるDeep Learning　～黒魔術を添えて～](http://qiita.com/t_Signull/items/f776aecb4909b7c5c116)
* [制限付きボルツマンマシンの初心者向けガイド](http://postd.cc/a-beginners-guide-to-restricted-boltzmann-machines/)
* [コンストラスティブ・ダイバージェンス法を用いた制限ボルツマンマシン(RBM)の実装](http://kivantium.hateblo.jp/entry/2015/12/01/000207)

##Functional Programming
* [Neural Networks, Types, and Functional Programming](http://colah.github.io/posts/2015-09-NN-Types-FP/)
