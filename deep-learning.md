#Deep Learning
* [Reducing the Dimensionality of Data with Neural Networks](http://www.cs.toronto.edu/~hinton/science.pdf) - Deep Learning の火付け役となった論文
* [ajtulloch/dnngraph](https://github.com/ajtulloch/dnngraph)
* [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)
* [Deep Learningと音声認識](https://research.preferred.jp/2015/07/deep-learning-speech-recognition/)
* [自然言語処理のためのDeep Learning](http://www.slideshare.net/yutakikuchi927/deep-learning-26647407)
* [Deep learning](http://www.slideshare.net/kazoo04/deep-learning-15097274)
* [一般向けのDeep Learning](http://www.slideshare.net/pfi/deep-learning-22350063)
* [andrewt3000/DL4NLP](https://github.com/andrewt3000/DL4NLP)
* [New ‘deep learning’ technique enables robot mastery of skills via trial and error](http://newscenter.berkeley.edu/2015/05/21/deep-learning-robot-masters-skills-via-trial-and-error/)
* [深層学習ライブラリのプログラミングモデル](http://www.slideshare.net/yutakashino/ss-56291783)
* [DeepLearningを使った実装を纏めてみた](http://nonbiri-tereka.hatenablog.com/entry/2015/12/17/004410)
* [DeepLearningの基礎と応用事例](http://www.slideshare.net/Takayosi/deep-learning-seminar)
* [ニューラルネットワークと深層学習](http://nnadl-ja.github.io/nnadl_site_ja/index.html)
* [DeepLearning〜使いこなすために知っておきたいこと〜](http://www.slideshare.net/Takayosi/miru2014-tutorial-deeplearning-37219713)
* [Deep Learningと画像認識～歴史・理論・実践～](http://www.slideshare.net/nlab_utokyo/deep-learning-40959442)
* [Deep Learningによる画像認識革命　ー歴史・最新理論から実践応用までー](http://www.slideshare.net/nlab_utokyo/deep-learning-49182466)
* [ディープラーニングの基礎技術と今後の課題・展望〜画像認識分野を中心に〜](http://www.slideshare.net/nlab_utokyo/20150930-53741757)
* [DeepLearningの過去と未来〜黒魔術からの脱却へ向けて〜](http://www.slideshare.net/nlab_utokyo/20150414seminar)
* [画像認識分野でのディープラーニングの研究動向](http://ibisml.org/archive/ibis2013/pdfs/ibis2013-okatani.pdf)
* [一般向けのDeep Learning](http://www.slideshare.net/pfi/deep-learning-22350063)
* [Deep Learningの基礎と応用](http://www.slideshare.net/beam2d/deep-learning-52872945)
* [Deep learning実装の基礎と実践](http://www.slideshare.net/beam2d/deep-learningimplementation)
* [日本神経回路学会セミナー「DeepLearningを使ってみよう！」資料](http://www.slideshare.net/KentaOono/deeplearning-52072261)
* [ディープラーニング チュートリアル（もしくは研究動向報告）](http://www.vision.is.tohoku.ac.jp/files/9313/6601/7876/CVIM_tutorial_deep_learning.pdf)
* [画像認識のための深層学習](http://www.slideshare.net/yomoyamareiji/ss-36982686)
* [なぜGPUはディープラーニングに向いているか](http://www.slideshare.net/NVIDIAJapan/gpu-51812528)
* [-SSIIの技術マップ- 過去•現在, そして未来 ［領域］認識](http://www.slideshare.net/hironobufujiyoshi/ssii-35734246)
* [論文輪読資料「A review of unsupervised feature learning and deep learning for time-series modeling」](http://www.slideshare.net/kaorunasuno/20150416-nasuno-ul)
* [Deep learning勉強会20121214ochi](http://www.slideshare.net/alembert2000/deep-learning20121214ochi)
* [Deep Learning 勉強会 (Chapter 7-12)](http://www.slideshare.net/alembert2000/deep-learning-chapter-712)
* [Learning Deep Architectures for AI （第 3 回 Deep Learning 勉強会資料; 松尾）](http://www.slideshare.net/alembert2000/learning-deep-architectures-for-ai-3-deep-learning)
* [Multimodal Deep Learning](http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Ngiam_399.pdf)
* [DeepLearningを使った実装を纏めてみた](http://nonbiri-tereka.hatenablog.com/entry/2015/12/17/004410)
* [生成モデルの Deep Learning](http://www.slideshare.net/beam2d/learning-generator)
* [Neural Networkでの失敗経験やアンチパターンを語る](http://nonbiri-tereka.hatenablog.com/entry/2016/03/10/073633)
* [自然言語処理における畳み込みニューラルネットワークを理解する](https://tkengo.github.io/blog/2016/03/11/understanding-convolutional-neural-networks-for-nlp/)
* [ニューラルネットと脳の違いが知りたくて](http://ntddk.github.io/2016/03/21/book-guide-for-computational-neuroscience/)
* [ディープラーニングと多様体学習](http://s0sem0y.hatenablog.com/entry/2016/03/17/155119)
* [Deep Learning技術の今](http://www.slideshare.net/beam2d/deep-learning20140130)
* [深層学習ライブラリ Keras](http://aidiary.hatenablog.com/entry/20160328/1459174455)
* [【ディープラーニング】最新のディープラーニングを一通りさらっておく](http://esu-ko.hatenablog.com/entry/2016/03/29/%E3%80%90%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%80%91%E6%9C%80%E6%96%B0%E3%81%AE%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3)
* [Deep Learningライブラリ「MXNet」のR版をKaggle Otto Challengeで実践してみた](http://keiku.hatenablog.jp/entry/2016/03/31/172456)
* [PTGH：機械学習パラメータチューニングをDeep LearningとMCMCで高速に最適化するフレームワーク](http://tjo.hatenablog.com/entry/2016/04/01/003000)
* [Music Language Modeling with Recurrent Neural Networks](http://yoavz.com/music_rnn/)
* [Deep Networks with Stochastic Depth](http://arxiv.org/abs/1603.09382#)
* [Generating Large Images from Latent Vectors](http://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors/)
* [Deep Reinforcement Learning from Self-Play in Imperfect-Information Games](http://arxiv.org/abs/1603.01121v1#)
* [Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks](http://www.mit.edu/~sze/eyeriss.html)
* [Theano: DeepLearning : 感情分析のための LSTM](http://rnn.classcat.com/2016/03/31/deeplearning-tutorial-lstm/)
* [deepjazz](https://jisungk.github.io/deepjazz/)
* [rewonc/pastalog](https://github.com/rewonc/pastalog)

##基礎
* **Back Propagation**
* [A Step by Step Backpropagation Example](http://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)
* [計算グラフの微積分：バックプロパゲーションを理解する](http://postd.cc/2015-08-backprop/)
* [誤差逆伝播法のノート](http://qiita.com/Ugo-Nama/items/04814a13c9ea84978a4c)
* ** Dropout **
* [Fast dropout training](http://jmlr.csail.mit.edu/proceedings/papers/v28/wang13a.pdf)
* [Dropoutの実装と重みの正則化](http://olanleed.hatenablog.com/entry/2013/12/03/010945)
* ** Batch Normalization **
* [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://arxiv.org/abs/1502.03167)
* [Embedding by Normalisation](http://arxiv.org/abs/1603.05197)

##BinaryNet
* [BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1](http://arxiv.org/abs/1602.02830)
* [BinaryNetとBinarized Deep Neural Network](http://tkng.org/b/2016/02/12/binarynet-and-binarized-neural-network/)
* [全探索によるニューラルネットワーク最適化の実験](http://kivantium.hateblo.jp/entry/2016/03/14/234939)

##Tools & Tutorials
* [Theano](http://deeplearning.net/software/theano/)
  * [Tutorial](http://deeplearning.net/software/theano/tutorial/)
  * [Theanoによる自己符号化器の実装](http://aidiary.hatenablog.com/entry/20151203/1449146680)
  * [DeepLearning Documentation](http://deeplearning.net/tutorial/contents.html)
  * [Theano の 基本メモ](http://qiita.com/mokemokechicken/items/3fbf6af714c1f66f99e9)
  * [Theano 解説](http://d.hatena.ne.jp/saket/20121207/1354867911)

###Chainer
> 制御構造はすべて Python のものがそのままつかえます。Chainer は、実際に Python のコードを用いて入力配列に何の処理が適用されたかだけを記憶しておき、それを誤差逆伝播の実行に使います。

[Deep Learning のフレームワーク Chainer を公開しました](https://research.preferred.jp/2015/06/deep-learning-chainer/)

* [Chainer](http://chainer.org/) - 公式ドキュメント
* [Chainer – A flexible framework of neural networks](http://docs.chainer.org/en/stable/)
* [Chainerの使い方と自然言語処理への応用](http://www.slideshare.net/unnonouno/chainer-55494686)
* [Chainerを使う場合の全体の流れ](http://www.slideshare.net/ryokuta/chainer-v15-view2015/29)
* [Chainerで顔イラストの自動生成](http://qiita.com/mattya/items/e5bfe5e04b9d2f0bbd47)
* [Chainerで学習した対話用のボットをSlackで使用+Twitterから学習データを取得してファインチューニング](http://qiita.com/GushiSnow/items/79ca7deeb976f50126d7)
* [Chainerでファインチューニングするときの個人的ベストプラクティス](http://qiita.com/tabe2314/items/6c0c1b769e12ab1e2614)
* [【機械学習】ディープラーニング フレームワークChainerを試しながら解説してみる。](http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412)
* [【ディープラーニング】ChainerでAutoencoderを試して結果を可視化してみる。](http://qiita.com/kenmatsu4/items/99d4a54d5a57405ecaf8)
* [Chainer入門と最近の機能](http://www.slideshare.net/unnonouno/chainer-56292907)
* [ChainerとRNNと機械翻訳](http://qiita.com/odashi_t/items/a1be7c4964fbea6a116e)
* [mattya/chainer-DCGAN](https://github.com/mattya/chainer-DCGAN)
* [Chainerを用いたマルウェア検出への取り組みについて](http://olanleed.hatenablog.com/entry/2015/12/13/215246)
* [ニューラルネットワークを用いたランク学習(ChainerによるRankNetの実装)](http://qiita.com/sz_dr/items/0e50120318527a928407)
* [Chainerを使った多層パーセプトロン：関数フィッティング](http://qiita.com/nga_m0m0/items/4191c0f847d1c749fd84)
* [Chainerのテスト環境とDockerでのCUDAの利用](http://www.slideshare.net/unnonouno/chainerdockercuda)
* [深層学習フレームワークChainerの特徴](http://www.slideshare.net/unnonouno/chainer-59664785)
* [DQNの生い立ち＋Deep Q-NetworkをChainerで書いた](http://qiita.com/Ugo-Nama/items/08c6a5f6a571335972d5)

###Tensor Flow
* [TensorFlow](http://tensorflow.org/)
* [Neural Networkをちょっとかじった人のための、はじめてのTensorFlow](http://qiita.com/sergeant-wizard/items/55256ac6d5d8d7c53a5a)
* [TensorFlow 畳み込みニューラルネットワークで手書き認識率99.2%の分類器を構築](http://qiita.com/haminiku/items/36982ae65a770565458d)
* [TensorFlow ってなんだろ](http://qiita.com/shuhei_f/items/5ba61fff4e47e073c24f)
* [TensorFlowとTensorBoardでニューラルネットワークを可視化](http://qiita.com/sergeant-wizard/items/fdf4d64a0d221a81da34)
* [TensorFlowのコード分割の考え方](http://qiita.com/sergeant-wizard/items/98ce0993a195475fd7a9)
* [TensorFlow紹介文の適当和訳 ななめ読み用](http://qiita.com/tomo_makes/items/af23c1ac0d94b764da55)
* [TesnsorFlowで深層学習像が変わる？少し触って思った事](http://qiita.com/n_kats_/items/ba7f19701e39d8ff1f6c)
* [TensorFlowを社内向けにざっくりLTして回帰した(＋資料とか)](http://yamitzky.hatenablog.com/entry/2015/11/11/204416)
* [nivwusquorum/tensorflow-deepq](https://github.com/nivwusquorum/tensorflow-deepq)
* [TensorFlowチュートリアル - TensorFlowメカニクス101（翻訳）](http://qiita.com/KojiOhki/items/0640d01029371d6ae092)
* [TensorFlowチュートリアル - 画像認識（翻訳）](http://qiita.com/KojiOhki/items/dab6922b6cd7b990c002)
* [TensorFlowをscikit-learnライクに使えるskflow](http://qiita.com/icoxfog417/items/9d052b556bd8a4074e4a)
* [TensorFlowでのMNIST学習結果を、実際に手書きして試す](http://d.hatena.ne.jp/sugyan/20151124/1448292129)
* [Googleから出た機械学習ライブラリTensorFlowのインストール](http://qiita.com/bohemian916/items/62ef3fe4d90745cc92f6)
* [Distributed TensorFlowの話](http://qiita.com/kazunori279/items/981a8a2a44f5d1172856)
* [TFLearn: Deep learning library featuring a higher-level API for TensorFlow.](http://tflearn.org/)

##事例
* [ディープラーニングによる画像認識と応用事例](http://www.slideshare.net/Takayosi/nvidia-51814334)
* [ディープラーニングでおそ松さんの六つ子は見分けられるのか 〜実施編〜](http://bohemia.hatenablog.com/entry/2015/11/22/174603)
* [TensorFlowによるディープラーニングで、アイドルの顔を識別する](http://d.hatena.ne.jp/sugyan/20160112/1452558576)
* [ディープラーニングであり得そうな間取り画像を生成させてみる](http://nextdeveloper.hatenablog.com/entry/2016/02/26/211332)
* [ニコニコ動画の公開コメントデータをDeep Learningで解析する](http://qiita.com/ixixi/items/a3d56b2db6e09249a519)
* [【エヴァンゲリオン】アスカっぽいセリフをDeepLearningで自動生成してみる](http://qiita.com/S346/items/24e875e3c5ac58f55810)
* [深層学習でニュース記事を分類する](http://qiita.com/hogefugabar/items/c27ed578717c5e7288c0)
* [深層学習でツイートの感情分析](http://qiita.com/hogefugabar/items/93fcb2bc27d7b268cbe6)
* [Chainerを使ってコンピュータにイラストを描かせる](http://qiita.com/rezoolab/items/5cc96b6d31153e0c86bc)
* [画風を変換するアルゴリズム](https://research.preferred.jp/2015/09/chainer-gogh/)
* [ディープラーニングを使って商品カテゴリの分類をしてみました](https://speakerdeck.com/shiozaki/classify-fashion-items-by-using-deep-learning)
* [TensorFlowでのDeep Learningによるアイドルの顔識別 のためのデータ作成](http://d.hatena.ne.jp/sugyan/20160328/1459094743)
* [Machine Learning with Financial Time Series Data](https://cloud.google.com/solutions/machine-learning-with-financial-time-series-data)
* [Chainerで画像のキャプション生成](http://qiita.com/dsanno/items/b237482087207d0364c3)
* [Deep3D: Automatic 2D-to-3D Video Conversion with CNNs](http://dmlc.ml/mxnet/2016/04/04/deep3d-automatic-2d-to-3d-conversion-with-CNN.html)
* [VISUAL LOCALISATION](http://mi.eng.cam.ac.uk/projects/relocalisation/)

##NN
* [jbarrow/LambdaNet](https://github.com/jbarrow/LambdaNet)
* [alpmestan/hnn](https://github.com/alpmestan/hnn)
* [Neural networks in Haskell (Lynn)](https://twitter.com/mcarberg/status/664750004742000640)
* [Haskellでニューラルネットワーク](http://imokuri123.com/blog/2015/07/neural-network-in-haskell.html)
* [数式で書き下す Maxout Networks](http://blog.yusugomori.com/post/133257383300/%E6%95%B0%E5%BC%8F%E3%81%A7%E6%9B%B8%E3%81%8D%E4%B8%8B%E3%81%99-maxout-networks)
* [ニューラルネットワーク、多様体、トポロジー](http://qiita.com/KojiOhki/items/af2241027b00f892d2bd)
* [ConvnetJS demo: toy 2d classification with 2-layer neural network](http://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html)
* [ロジスティック回帰 (勾配降下法 / 確率的勾配降下法) を可視化する](http://sinhrks.hatenablog.com/entry/2014/11/24/205305)

##CNN
* [3D Visualization of a Convolutional Neural Network](http://scs.ryerson.ca/~aharley/vis/conv/)
* [畳み込みニューラルネット](http://www.slideshare.net/ssuser726f56/joi-51681753)
* [数式で書き下す Convolutional Neural Networks (CNN)](http://blog.yusugomori.com/post/129688163130/%E6%95%B0%E5%BC%8F%E3%81%A7%E6%9B%B8%E3%81%8D%E4%B8%8B%E3%81%99-convolutional-neural-networks-cnn)
* [深層畳み込みニューラルネットワークを用いた画像スケーリング](http://postd.cc/image-scaling-using-deep-convolutional-neural-networks-part1/)
* [ディープラーニングにおける様々な物体領域検出のアプローチ方法(R-CNN)](http://qiita.com/t-hiroyoshi/items/e9def50ba2c2249db04b)
* [A Theory of Generative ConvNet](http://www.stat.ucla.edu/~ywu/GenerativeConvNet/main.html)
* [自然言語処理における畳み込みニューラルネットワークを理解する](http://tkengo.github.io/blog/2016/03/11/understanding-convolutional-neural-networks-for-nlp/)
* [Deep Learningライブラリ{mxnet}のR版でConvolutional Neural Networkをサクッと試してみた](http://tjo.hatenablog.com/entry/2016/03/29/180000)

##RNN
* [JPMoresmau/rnn](https://github.com/JPMoresmau/rnn)
* [リカレントニューラルネットなぜ強い？](http://d.hatena.ne.jp/mamoruk/20151209/p1)
* [Recurrent Neural Networks](http://www.slideshare.net/beam2d/pfi-seminar-20141030rnn)
* [Recurrent Neural Networks Tutorial, Part 1 – Introduction to RNNs](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)
* [Playing with Recurrent Neural Networks in Haskell](http://jpmoresmau.blogspot.jp/2015/08/playing-with-recurrent-neural-networks.html)
* [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
* [TensorFlowチュートリアル - リカレント・ニューラルネットワーク（翻訳）](http://qiita.com/KojiOhki/items/149f96bd98973bd219ac)
* [ChainerとRNNと機械翻訳](http://qiita.com/odashi_t/items/a1be7c4964fbea6a116e)
* [ニューラルネットワークで時系列データの予測を行う](http://qiita.com/icoxfog417/items/2791ee878deee0d0fd9c)
* [再帰型ニューラルネットワーク講義シリーズ・第1部: RNN入門](http://blog.moji.ai/2016/01/%E5%86%8D%E5%B8%B0%E5%9E%8B%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%AC%9B%E7%BE%A9%E3%82%B7%E3%83%AA%E3%83%BC%E3%82%BA%E3%83%BB%E7%AC%AC1/)
* [論文輪読資料「Gated Feedback Recurrent Neural Networks」](http://www.slideshare.net/kurotaki_weblab/gated-feedback-recurrent-neural-networks)
  * [最近のDeep Learning (NLP) 界隈におけるAttention事情](http://www.slideshare.net/yutakikuchi927/deep-learning-nlp-attention)

###LSTM
* [LSTMネットワークの概要](http://qiita.com/KojiOhki/items/89cd7b69a8a6239d67ca)
* [わかるLSTM ～ 最近の動向と共に](http://qiita.com/t_Signull/items/21b82be280b46f467d1b)
* [Chainerで学ぶLSTM](http://kivantium.hateblo.jp/entry/2016/01/31/222050)
* [RECURRENT NEURAL NETWORK REGULARIZATION](http://arxiv.org/pdf/1409.2329v4.pdf)
* [Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks](http://arxiv.org/abs/1503.00075)
* [Long Short-Term Memory in Recurrent Neural Networks](http://www.felixgers.de/papers/phd.pdf)

##Autoencoder
* [論文輪読資料「Why regularized Auto-Encoders learn Sparse Representation?」DL Hacks](http://www.slideshare.net/kurotaki_weblab/why-regularized-autoencoders-learn-sparse-representationdl-hacks)
* [ディープラーニング勉強会 AutoEncoder](http://qiita.com/piyo7/items/60576759430910ffe5be)
* [word2vec](https://code.google.com/p/word2vec/)
* [【ディープラーニング】ChainerでAutoencoderを試して結果を可視化してみる。](http://qiita.com/kenmatsu4/items/99d4a54d5a57405ecaf8)
* [Doc2Vecとk-meansで教師なしテキスト分類](http://qiita.com/shima_x/items/196e8d823412e45680e9)
* [Semi-Supervised Autoencoders for Predicting Sentiment Distributions（第 5 回 Deep Learning 勉強会資料; ダヌシカ）](http://www.slideshare.net/alembert2000/ssrae-sa)

##RBM
* [aeyakovenko/rbm](https://github.com/aeyakovenko/rbm)
* [RBMから考えるDeep Learning　～黒魔術を添えて～](http://qiita.com/t_Signull/items/f776aecb4909b7c5c116)
* [制限付きボルツマンマシンの初心者向けガイド](http://postd.cc/a-beginners-guide-to-restricted-boltzmann-machines/)
* [コンストラスティブ・ダイバージェンス法を用いた制限ボルツマンマシン(RBM)の実装](http://kivantium.hateblo.jp/entry/2015/12/01/000207)

##Functional Programming
* [Neural Networks, Types, and Functional Programming](http://colah.github.io/posts/2015-09-NN-Types-FP/)
